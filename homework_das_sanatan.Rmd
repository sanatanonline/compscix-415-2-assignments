---
title: "COMPSCIX 415.2 Homework 5/Midterm"
author: "Sanatan Das"
date: "March 2, 2018"
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}\LARGE\includegraphics[width=5cm]{logo.png}\\[\bigskipamount]}
  - \posttitle{\end{center}}
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 2
  pdf_document:
    toc: yes
    toc_depth: '2'
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Code and Documents Git Repository

https://github.com/sanatanonline/compscix-415-2-assignments

# Load packages (prerequisites)
```{r load_packages, warning=FALSE, message=FALSE}
library(tidyverse)
library(nycflights13)
library(dplyr)
```

# RStudio and R Markdown (3 points)

**1. Use markdown headers in your document to clearly separate each midterm question and add a table of contents to your document.**

**Answer**

The following is the code used for RMD header and to add a table of contents in the document.

```{r header, eval=FALSE, echo=TRUE}
title: "COMPSCIX 415.2 Homework 5/Midterm"
author: "Sanatan Das"
date: "March 2, 2018"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 2
  pdf_document:
    toc: yes
    toc_depth: '2'
```


# The tidyverse packages (3 points)

**1. Can you name which package is associated with each task below?**

* **Plotting -**
   
* **Data munging/wrangling -**
   
* **Reshaping (speading and gathering) data -**
   
* **Importing/exporting data -**
   

**Answer:**

The below are the packages associated with each task below.

* Plotting - Plotting is done mainly using *ggplot2* package which is a core member of *tidyverse* package.
   
* Data munging/wrangling - This mainly uses base R packages, *tibble* (which is a core member of *tidyverse* package). We have used the datasets from *nycflights13* package.
   
* Reshaping (speading and gathering) data - Reshaping of data is done using the functions of base R, *tibble* (which is a core member of *tidyverse* package) and *dplyr* packages.
   
* Importing/exporting data - Import and export of data mainly uses the functions of *readR* package which is a core member of *tidyverse* package.
   

**2. Now can you name two functions that you’ve used from each package that you listed above for these tasks?**

* **Plotting -**

* **Data munging/wrangling -**

* **Reshaping data -**

* **Importing/exporting data (note that readRDS and saveRDS are base R functions) -**

**Answer:**

The following are functions used from the packages listed above for the tasks.

* Plotting -

* Data munging/wrangling -

* Reshaping data -

* Importing/exporting data (note that readRDS and saveRDS are base R functions) -


# R Basics (1.5 points)


**1. Fix this code with the fewest number of changes possible so it works:**

```{r var1, eval=FALSE, echo=TRUE}
My_data.name___is.too00ooLong! <- c( 1 , 2 , 3 )
```

**Answer**

```{r var2, eval=TRUE, echo=TRUE}
My_data.name___is.too00ooLong <- c( 1 , 2 , 3 )
My_data.name___is.too00ooLong
```

Explanation: '!' is not allowed in a variable name. If the code is executed, R gives the below error.
Error: unexpected '!' in "My_data.name___is.too00ooLong!"



**2. Fix this code so it works:**

```{r var3, eval=FALSE, echo=TRUE}
my_string <- C('has', 'an', 'error', 'in', 'it)
```

**Answer**

There are two issues in the above code.

* my_string <- C(, in this code "C" is in uppercase whereas it should be lowercase. R is case sensitive.
* The last element is not enslosed by quote. So R can not parse it and throws parse error.

The correct code is as below.


```{r var4, eval=TRUE, echo=TRUE}
my_string <- c('has', 'an', 'error', 'in', 'it')
my_string
```

**3. Look at the code below and comment on what happened to the values in the vector.**

```{r var5, eval=TRUE, echo=TRUE}
my_vector <- c(1, 2, '3', '4', 5)
my_vector
```

**Answer**

A vector is a sequence of data elements of the same basic type in R. So it automatically converts the numbers to character String.


# Data import/export (3 points)

**1. Download the rail_trail.txt file from Canvas (in the Midterm Exam section here) and successfully import it into R. Prove that it was imported successfully by including your import code and taking a glimpse of the result.**

**Answer**

```{r var6}

# Read from rail_trail.txt file
rail_trail <- read_csv("C:/view/opt/apps/git/R/compscix-415-2-assignments/rail_trail.txt")

# glimpse rail_trail
glimpse(rail_trail)

```

**2. Export the file into an R-specific format and name it “rail_trail.rds”. Make sure you define the path correctly so that you know where it gets saved. Then reload the file. Include your export and import code and take another glimpse.**

**Answer**


```{r var7}

# Read from rail_trail.txt file
rail_trail <- read_csv("C:/view/opt/apps/git/R/compscix-415-2-assignments/rail_trail.txt")

# glimpse rail_trail
glimpse(rail_trail)

# Write to rail_trail.rds
saveRDS(rail_trail, "rail_trail.rds")

# load rail_trail.rds
new_rail_trail = readRDS("C:/view/opt/apps/git/R/compscix-415-2-assignments/rail_trail.rds")
  
# glimpse new_rail_trail
glimpse(new_rail_trail) 

```

# Visualization (6 points)

**1. Critique this graphic: give only three examples of what is wrong with this graphic. Be concise.**

**Answer**

This graphic has multiple issues which creates wrong impressions of the data. The major three wrong representations are:

* This is not a standard statistical chart/plot representation. The numeric values definitely does not match the size of the images.

* What are those numbers represents? Are percentage/total number of responders or what...not clear.

* What are those colors represent? No legend. Not clear.

**2. Reproduce this graphic using the diamonds data set.**

**Answer**

```{r var22, eval=FALSE, echo=TRUE}

mutate(diamonds, price_per_carat <- price / carat)

ggplot(data = diamonds) +
  geom_boxplot(aes(x = cut, y = carat, fill = color)) +
  coord_flip()

```



**3. The previous graphic is not very useful. We can make it much more useful by changing one thing about it. Make the change and plot it again.**

**Answer**

Write it.


# Data munging and wrangling (6 points)

**1. Is this data “tidy”? If yes, leave it alone and go to the next problem. If no, make it tidy. Note: this data set is called table2 and is available in the tidyverse package. It should be ready for you to use after you’ve loaded the tidyverse package.**

```{r var8, eval=TRUE, echo=TRUE}
table2

```

**Answer**

This is not a tidy data. This datast intermingles the values of population and cases in the same columns. As a result, we would need to untangle the values whenever we want to work with each variable separately.

The *key* column contains only keys (and not just because the column is labelled *key*). Conveniently, the *value* column contains the values associated with those keys.

We can use the *spread()* function to tidy this layout. So the tidy form of the dataset would be like below.

```{r var20, eval=TRUE, echo=TRUE}
spread(table2, type, count)
```

**2. Create a new column in the diamonds data set called price_per_carat that shows the price of each diamond per carat (hint: divide). Only show me the code, not the output.**

We can do this using code below.

```{r var21, eval=FALSE, echo=TRUE}

mutate(diamonds, price_per_carat <- price / carat)

```


**3. For each cut of diamond in the diamonds data set, how many diamonds, and what proportion, have a price > 10000 and a carat < 1.5? There are several ways to get to an answer, but your solution must use the data wrangling verbs from the tidyverse in order to get credit.**

* **Do the results make sense? Why?**

* **Do we need to be wary of any of these numbers? Why?**

**Answer**

```{r var22, eval=TRUE, echo=TRUE}

filter(diamonds, price > 10000, carat < 1.5)

diamonds2 <- diamonds %>%
  group_by(cut) %>%
  summarise(prop = sum(price > 10000, carat < 1.5)/ n()) %>%
  arrange(cut)

print(tbl_df(diamonds2))

```

# EDA (6 points)

**Take a look at the txhousing data set that is included with the ggplot2 package and answer these questions:**

**1. During what time period is this data from?**

**2. How many cities are represented?**

**3. Which city, month and year had the highest number of sales?**

**4. What kind of relationship do you think exists between the number of listings and the number of sales? Check your assumption and show your work.**

**5. What proportion of sales is missing for each city?**

**6. Looking at only the cities and months with greater than 500 sales:**

* **Are the distributions of the median sales price (column name median), when grouped by city, different? The same? Show your work.**

* **Any cities that stand out that you’d want to investigate further?**

* **Why might we want to filter out all cities and months with sales less than 500?**


**Answer**

To do the EDA on *txhousing* data, take a quick look at the table. We can use ?txhousing for help to understand the variabls.

```{r var9, eval=TRUE, echo=TRUE}
txhousing
```

From the above result, we see its a dataset of 9 variables with 8602 observations. Now we will do the analysis to answer the above questions.

**1. During what time period is this data from?**

```{r var10, eval=TRUE, echo=TRUE}

arrange(txhousing, year, month)

arrange(txhousing, desc(year), desc(month))

```

From the above results, we see that the data is collected monthly from Jan 2000 to July 2015

**2. How many cities are represented?**


```{r var11, eval=TRUE, echo=TRUE}

count(unique(txhousing[,1]))

```

There are 46 cities represented in *txhousing* dataset.

**3. Which city, month and year had the highest number of sales?**

In this dataset *sales* variable represents the **number of sales.** So we arrange the dataset in descending order by sales.

```{r var12, eval=TRUE, echo=TRUE}

arrange(txhousing, desc(sales))

```

In the above result we see, Houston had the highest number of sales 8945 in July (month 7), 2015. This had maximum volume (total value of sales) too.

**4. What kind of relationship do you think exists between the number of listings and the number of sales? Check your assumption and show your work.**

```{r var13, eval=TRUE, echo=TRUE}

ggplot(data = txhousing) + 
  geom_point(mapping = aes(x = listings, y = sales))

```

In the above scatter plot we see that there is a relationship between the number of listings and the number of sales. When there is more number of listings, number of sales increases. The below plot confirms the trend.

```{r var14, eval=TRUE, echo=TRUE}

ggplot(data = txhousing) + 
  geom_smooth(mapping = aes(x = listings, y = sales))

```

**5. What proportion of sales is missing for each city?**

We can find out the proportion of sales is missing for each city using following code.

```{r var15, eval=TRUE, echo=TRUE}

missing_sales_prop_per_city <- txhousing %>%
  group_by(city) %>%
  summarise(prop = sum(is.na(sales))/ n()) %>%
  arrange(city)

print(tbl_df(missing_sales_prop_per_city), n=46)

```


**6. Looking at only the cities and months with greater than 500 sales:**

* **Are the distributions of the median sales price (column name median), when grouped by city, different? The same? Show your work.**

* **Any cities that stand out that you’d want to investigate further?**

* **Why might we want to filter out all cities and months with sales less than 500?**

First filter out the months with less than or equal to 500 as we are going to look at only the cities and months with greater than 500 sales.

```{r var16, eval=TRUE, echo=TRUE}

newtxhousing <- filter(txhousing, sales > 500)

newtxhousing


```

And we see there are such 1883 records.

```{r var17, eval=TRUE, echo=TRUE}

ggplot(newtxhousing, aes(date, median)) + 
  geom_line(aes(group = city), alpha = 1/2) +
  geom_line(stat = "summary", fun.y = "mean", colour = "red")

```

Looking at the above plot, we can see the some of the cities (I guess big cities like Houston) has more sales than other cities.


```{r var18, eval=TRUE, echo=TRUE}

#OutVals = boxplot(newtxhousing$sales)$out
#which(newtxhousing %in% OutVals)

OutVals <- tibble(boxplot(newtxhousing$median, plot=FALSE)$out)

OutVals[1]

outliers_cities <- subset(newtxhousing, newtxhousing$median %in% OutVals)

outliers_cities

filter(newtxhousing, newtxhousing$median %in% OutVals[1])

```

First lets see the records having sales less than 500.

```{r var19, eval=TRUE, echo=TRUE}

newtxhousing <- filter(txhousing, sales < 500)

ggplot(data = newtxhousing) + 
  geom_smooth(mapping = aes(x = newtxhousing$year, y = newtxhousing$sales))


```




# Git and Github (1.5 points)

**To demonstrate your use of git and Github, at the top of your document put a hyperlink to your Github repository.**


**Answer**

All the work is pushed to Github repository. The repository URL is below.

https://github.com/sanatanonline/compscix-415-2-assignments


 \center --------------------------------------------------------------------------------------------------------- \center


\center __End of Homework 5/Midterm__ \center





   