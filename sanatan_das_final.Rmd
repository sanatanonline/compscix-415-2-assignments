---
title: "COMPSCIX 415.2 Homework 9/Final"
author: "Sanatan Das"
date: "March 29, 2018"
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}\LARGE\includegraphics[width=5cm]{logo.png}\\[\bigskipamount]}
  - \posttitle{\end{center}}
output:
  pdf_document:
    toc: yes
  html_document:
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage


# Code and Documents Git Repository

All the work can be found in the below Git repository location:

https://github.com/sanatanonline/compscix-415-2-assignments


# Load packages (prerequisites to run the code in this document)
```{r load_packages, warning=FALSE, message=FALSE}

# load the packages
library(tidyverse)
library(broom)
library(rpart)
library(ROCR)
library(partykit)
library(modelr)
library(randomForest)

```

# Bootstrapping

## Load the Titanic dataset

**1. Follow these steps:**

* **Load the train.csv dataset into R.**

* **Convert all character columns into unordered factors.**

* **Convert the Survived column into an unordered factor because it is loaded as an integer by default.**

* **Take a glimpse of your data to confirm that all of the columns were converted correctly.**

**We will use this same dataset for this entire assignment.**


**Answer:**

```{r var1, eval=TRUE, echo=TRUE}

# load train.csv
train = read_csv("C:/view/opt/apps/git/compscix-415-2-assignments/titanic/train.csv")
# glimpse train
glimpse(train) 
# add the factors
train <- train %>% 
  mutate(Survived_Category = case_when(Survived == 1 ~ 'Yes',
                                       Survived == 0 ~ 'No')) %>% 
  mutate(Pclass_Category = case_when(Pclass == 1 ~ 'First',
                                     Pclass == 2 ~ 'Second',
                                     Pclass == 3 ~ 'Third'))
train$Name <- as.factor(train$Name)
train$Sex <- as.factor(train$Sex)
train$Ticket <- as.factor(train$Ticket)
train$Cabin <- as.factor(train$Cabin)
train$Embarked <- as.factor(train$Embarked)
# glimpse train again
glimpse(train)

```

Now all the character columns are converted to unordered factors.

## Create bootstrap samples

**2. Use the code below to take 100 bootstrap samples of your data. Confirm that the result is a tibble with a list column of resample objects - each resample object is a bootstrap sample of the titanic dataset.**

```{r var2, eval=FALSE, echo=TRUE}

library(tidyverse)
library(modelr)
titanic_boot <- bootstrap(data = ____, n = ___)

```

**Answer:**

Let's fill in the code.

```{r var3, eval=TRUE, echo=TRUE}

# create bootstrap
titanic_boot <- bootstrap(data = train, n = 100)
#see the type of titanic_boot
class(titanic_boot)
# see titanic_boot
titanic_boot
# see the type of elements
class(titanic_boot$strap[[1]])

```

We can see that a tibble of 100 rows is created with a list column of resample objects - each resample object is a bootstrap sample of the titanic dataset.

## Verify bootstrap samples

**3. Confirm that some of your bootstrap samples are in fact bootstrap samples (meaning they should have some rows that are repeated). You can use the *n_distinct()* function from dplyr to see that your samples have different numbers of unique rows. Use the code below to help you extract some of the resample objects from the strap column (which is an R list), convert them to tibbles, and then count distinct rows. Use the code below, no changes necessary.**

```{r var4, eval=FALSE, echo=TRUE}

# since the strap column of titanic_boot is a list, we can
# extract the resampled data using the double brackets [[]],
# and just pick out a few of them to compare the number of
# distinct rows
as.tibble(titanic_boot$strap[[1]]) %>% n_distinct()
as.tibble(titanic_boot$strap[[2]]) %>% n_distinct()
as.tibble(titanic_boot$strap[[3]]) %>% n_distinct()

```

**Answer:**

Let's run the code.

```{r var5, eval=TRUE, echo=TRUE}

# since the strap column of titanic_boot is a list, we can
# extract the resampled data using the double brackets [[]],
# and just pick out a few of them to compare the number of
# distinct rows

# not distinct rows
as.tibble(titanic_boot$strap[[1]]) %>% count()
# distinct rows
as.tibble(titanic_boot$strap[[1]]) %>% n_distinct()
# not distinct rows
as.tibble(titanic_boot$strap[[2]]) %>% count()
# distinct rows
as.tibble(titanic_boot$strap[[2]]) %>% n_distinct()
# not distinct rows
as.tibble(titanic_boot$strap[[3]]) %>% count()
# distinct rows
as.tibble(titanic_boot$strap[[3]]) %>% n_distinct()

```

From the above, we see that every tibble has 891 rows (length of the dataset) but the number distinct rows are less than that. That means some rows are repeated.

## Demonstration of Central Limit Theorem

**4. Now, let’s demonstrate the Central Limit Theorem using the Age column. We’ll iterate through all 100 bootstrap samples, take the mean of Age, and collect the results.**

* **We will define our own function to pull out the mean of Age from each bootstrap sample and**

* **create our own for loop to iterate through.**

**Use the code below and fill in the blanks.**

```{r var6, eval=FALSE, echo=TRUE}

age_mean <- function(___) {
data <- as.tibble(___) # convert input data set to a tibble
mean_age <- mean(data$Age, na.rm = TRUE) # take the mean of Age, remove NAs
return(____) # return the mean value of Age from data
}
# loop through the 100 bootstrap samples and use the age_mean()
# function
all_means <- rep(NA, 100)
# start the loop
for(i in 1:___) {
all_means[_] <- age_mean(titanic_boot$strap[[i]])
}
# take a look at some of the means you calculated from your samples
head(all_means)
# convert to a tibble so we can use if for plotting
all_means <- tibble(all_means = all_means)

```

**Answer:**

Let's fill in the code.

```{r var7, eval=TRUE, echo=TRUE}

# function for calculating mean age
age_mean <- function(input) {
data <- as.tibble(input) # convert input data set to a tibble
mean_age <- mean(data$Age, na.rm = TRUE) # take the mean of Age, remove NAs
return(mean_age) # return the mean value of Age from data
}
# loop through the 100 bootstrap samples and use the age_mean()
# function
all_means <- rep(NA, 100)
# start the loop
for(i in 1:100) {
all_means[i] <- age_mean(titanic_boot$strap[[i]])
}
# take a look at some of the means you calculated from your samples
head(all_means)
# convert to a tibble so we can use if for plotting
all_means <- tibble(all_means = all_means)

```

**5. Plot a histogram of *all_means*.**

The plot is as below.

```{r var8, eval=TRUE, echo=TRUE}
# Plot Histogram
ggplot(data=all_means) +
  geom_histogram(aes(x = all_means), binwidth = 0.5) +
  theme_bw()
# Plot Density Curve (additiona)
ggplot(data=all_means) +
  geom_density(aes(x = all_means)) +
  theme_bw()

```

**6. Find the standard error of the sample mean of Age using your boostrap sample means. Compare the empirical standard error to the theoretical standard error.**

**Answer:**

Standard Error (theoritical and empirical)

```{r var9, eval=TRUE, echo=TRUE}

stderr <- function(x){
  return(sqrt(var(x,na.rm=TRUE)/length(na.omit(x))))
}
# theoritical standard error
stderr_1 <- stderr(train$Age)
stderr_1

# empirical standard error
stderr_2 <- stderr(all_means)
stderr_2

```

Find out the standard error without resampling (theoritical standard error using lm).

```{r var10, eval=TRUE, echo=TRUE}

# theoritical standard error
lm_fit <- lm(Survived ~ Age, data = train)
tidy(lm_fit) %>% select(term, std.error)

```

Now find out bootstrap standard error (using lm).

```{r var11, eval=TRUE, echo=TRUE}
# function to take a bootstrap sample, 
# fit the linear regression model, 
# and return the coefficients
boot_fun <- function(data_samp) {
  lm_samp <- lm(Survived ~ Age, data = data_samp)
  return(tidy(lm_samp)$estimate)
}
# create empty vectors to save the output
intercepts <- rep(NA, 100)
slopes <- rep(NA, 100)
for(i in 1:100) { # loop over the sequence 1,2,3,...1000
  coefs <- boot_fun(titanic_boot$strap[[i]])
  intercepts[i] <- coefs[1] # save the output
  slopes[i] <- coefs[2] # save the output
}
# convert to tibble
all_results <- tibble(intercepts, slopes)
# bootstrap standard error
all_results %>% summarize(se_intercept = sd(intercepts), 
                          se_slope = sd(slopes))

```

From the above result, we can see that the bootstrap standard error is less compared to standard error without resampling (theoritical standard error)


# Random forest

## Split the dataset

**On the last homework, we fit a decision tree to the Titanic data set to predict the probability of survival given the features. This week we’ll use the random forest and compare our results to the decision tree.**

**1. Randomly split your data into training and testing using the code below so that we all have the same sets.**

```{r var12, eval=FALSE, echo=TRUE}

set.seed(987)
model_data <- resample_partition(____, c(test = 0.3, train = 0.7))
train_set <- as.tibble(model_data$____)
test_set <- as.tibble(model_data$____)

```

**Answer:**

```{r var13, eval=TRUE, echo=TRUE}

set.seed(987)
# factor the Survived column
train$Survived <- as.factor(train$Survived)
model_data <- resample_partition(train, c(test = 0.3, train = 0.7))
train_set <- as.tibble(model_data$train)
train_set
test_set <- as.tibble(model_data$test)
test_set

```

## Fit a decision tree

**2. Fit a decision tree to *train_set* using the *rpart* package, and using *Pclass*, *Sex*, *Age*, *SibSp*, *Parch*, *Fare*, *Embarked* as the features.**

* **Plot the tree using the *partykit* package.**

* **What do you notice about this tree compared to the one from last week which only contained three features?**

**Answer:**

```{r var14, eval=TRUE, echo=TRUE}

# Last week's tree model
tree_mod_1 <- rpart(Survived_Category ~ Pclass_Category + Sex + Fare, data = train_set)
# plot the tree
plot(as.party(tree_mod_1))

# This week's new tree model
tree_mod <- rpart(Survived_Category ~ Pclass_Category + Sex + Age + SibSp + Parch + Fare + Embarked, data = train_set)
# plot the tree
plot(as.party(tree_mod))

```

**Observation:** From the above, plots we see that in last week's tree, we had just 3 features and created 6 leaf nodes or categories. This week, we are creating the tree model with 7 features which has created a tree with more levels and 8 leaf nodes. Not much change in Sex=female for First and Second class passengers.

## Fit a random forest

**3. Fit a random forest to *train_set* using the *randomForest* package, and using *Pclass*, *Sex*, *Age*, *SibSp*, *Parch*, *Fare*, *Embarked* as the features. We’ll use 500 trees and sample four features at each split. Use the code below and fill in the blanks.**

```{r var15, eval=FALSE, echo=TRUE}

library(randomForest)
rf_mod <- randomForest(____ ~ _____,
                        data = train_set,
                        ntrees = 500,
                        mtry = 4,
                        na.action = na.roughfix)

```

**Answer:**

Let's fill in the code.

```{r var16, eval=TRUE, echo=TRUE}

rf_mod <- randomForest(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
                        data = train_set,
                        ntrees = 500,
                        mtry = 4,
                        na.action = na.roughfix)

```

## Comparion of the performance of the decision tree with the random forest model


**4. Compare the performance of the decision tree with the random forest using the ROCR package and the AUC. Which model performs the best?**

**Here’s some code to get you started.**

```{r var17, eval=FALSE, echo=TRUE}

library(ROCR)
rf_preds <- predict(____, newdata = test_set, type = 'prob')[,2]
tree_preds <- predict(____, newdata = test_set)[,2]
pred_rf <- prediction(predictions = _____, labels = ____)
pred_tree <- prediction(predictions = _____, labels = ____)

```


**Answer:**

Let's fill in the code.

```{r var18, eval=TRUE, echo=TRUE}

# predit the test data
rf_preds <- predict(rf_mod, newdata = test_set, type = 'prob')[,2]
tree_preds <- predict(tree_mod, newdata = test_set)[,2]
# create the prediction objects for both models
pred_rf <- prediction(predictions = rf_preds, labels = test_set$Survived)
pred_tree <- prediction(predictions = tree_preds, labels = test_set$Survived_Category)
# calculate the AUC
auc_rf <- performance(pred_rf, measure = 'auc')
auc_tree <- performance(pred_tree, measure = 'auc')

```

Extract the AUC value of the models

```{r var19, eval=TRUE, echo=TRUE}

# extract the AUC value for random forest
auc_rf@y.values[[1]]
# extract the AUC value for decision tree
auc_tree@y.values[[1]]

```

We see that the AUC value is higher for random forest than decision tree model. That means, random forest is performing better than decision tree model.

## Plot the ROC curves

**5. Plot the ROC curves for the decision tree and the random forest above, on the same plot with a legend that differentiates and specifies which curve belongs to which model. Use the code below to get you started. Hints:**

* **You will have to modify the plot_roc() function to plot the two curves together with different colors and a legend.**

* **This is easier to do if the data for plotting the two curves are in one tibble. You can combine tibbles using the bind_rows() function.**

```{r var20, eval=FALSE, echo=TRUE}

# get the FPR and TPR for the logistic model
# recall that the ROC curve plots the FPR on the x-axis
perf_rf <- performance(pred_rf, measure = 'tpr', x.measure = 'fpr')
perf_rf_tbl <- tibble(perf_rf@x.values[[1]], perf_rf@y.values[[1]])
# Change the names of the columns of the tibble
names(perf_rf_tbl) <- c('fpr', 'tpr')
# get the FPR and TPR for the tree model
perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr')
perf_tree_tbl <- tibble(perf_tree@x.values[[1]], perf_tree@y.values[[1]])
# Change the names of the columns of the tibble
names(perf_tree_tbl) <- c('fpr', 'tpr')
# Plotting function for plotting a nice ROC curve using ggplot
plot_roc <- function(perf_tbl) {
p <- ggplot(data = perf_tbl, aes(x = fpr, y = tpr)) +
geom_line(color = 'blue') +
geom_abline(intercept = 0, slope = 1, lty = 3) +
labs(x = 'False positive rate', y = 'True positive rate') +
theme_bw()
return(p)
}

```

**Answer:**

Let's update the code and run it.

```{r var21, eval=TRUE, echo=TRUE}

# get the FPR and TPR for the random forest model
# recall that the ROC curve plots the FPR on the x-axis
perf_rf <- performance(pred_rf, measure = 'tpr', x.measure = 'fpr')
perf_rf_tbl <- tibble(perf_rf@x.values[[1]], perf_rf@y.values[[1]])
# Change the names of the columns of the tibble
names(perf_rf_tbl) <- c('fpr', 'tpr')
# get the FPR and TPR for the decision tree model
perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr')
perf_tree_tbl <- tibble(perf_tree@x.values[[1]], perf_tree@y.values[[1]])
# Change the names of the columns of the tibble
names(perf_tree_tbl) <- c('fpr', 'tpr')
# Plotting function for plotting a nice ROC curve using ggplot
plot_roc <- function(perf_tbl) {
p <- ggplot(data = perf_tbl, aes(x = fpr, y = tpr, group=Model, colour=Model)) +
geom_line() +
geom_abline(intercept = 0, slope = 1, lty = 3) +
labs(x = 'False positive rate', y = 'True positive rate') +
theme_bw()
return(p)
}
# Create ROC
t1 <- perf_rf_tbl %>% mutate(Model = 'Random Forest') 
t2 <- perf_tree_tbl %>% mutate(Model = 'Decision Tree')
t3 <- bind_rows(t1, t2)
plot_roc(t3)

```

**6. Answer these questions about the ROC curves:**

* **which model performs better: decision tree or random forest?**

* **what is the approximate false positive rate, for both the decision tree and the random forest, if we attain a true positive rate of approximately 0.75? Answers do not need to be exact - just ballpark it by looking at the plots.**

**Answer:**

From the ROC curves, we see that random forest is performing better than decision tree model.

**Random Forest**

To attain a True Positive Rate: 0.75, False Positive Rate: 0.14

**Decision Tree**

To attain a True Positive Rate: 0.75, False Positive Rate: 0.36

Let's create a cut-off matric to confirm this.

```{r var22, eval=TRUE, echo=TRUE}

# Create cutoffs matrix for Random Forest model
rf_cutoffs <- data.frame(cut=perf_rf@alpha.values[[1]], fpr=perf_rf@x.values[[1]], tpr=perf_rf@y.values[[1]])
rf_cutoffs <- rf_cutoffs[order(rf_cutoffs$tpr, decreasing = FALSE),]
head(subset(rf_cutoffs, tpr>=0.7))
# Create cutoffs matrix for Decision Tree model
tree_cutoffs <- data.frame(cut=perf_tree@alpha.values[[1]], fpr=perf_tree@x.values[[1]], tpr=perf_tree@y.values[[1]])
tree_cutoffs <- tree_cutoffs[order(tree_cutoffs$tpr, decreasing = FALSE),]
head(subset(tree_cutoffs, tpr>=0.6))

```

\center --------------------------------------------------------------------------------------------------------- \center


\center __End of COMPSCIX 415.2 Homework 9/Final__ \center




